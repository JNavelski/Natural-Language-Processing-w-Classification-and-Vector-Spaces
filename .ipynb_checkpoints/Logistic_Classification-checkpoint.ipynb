{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "import gc\n",
    "import jsonlines\n",
    "import requests\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "\n",
    "# gc.get_objects()\n",
    "# locals()\n",
    "# globals()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-noise",
   "metadata": {},
   "source": [
    "# Lab 1: Logistic Regression\n",
    "\n",
    "In this lab, we will be using twitter data to train and classify the sentiment of new tweets.  The sentiment we will be classifying will be if a tweet is happy or sad, and we will be using the Python Natural Language Tool Kit (NLTK) to do so.  The process to rigorously classify the sentiment of documents, in this case Tweets, is:\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item Import Functions and Data\n",
    "    \\item Prepare the Data\n",
    "    \\item Define a Sigmoid for Logistic Regression\n",
    "    \\item Define a Model for Logistic Regression using a Cost Function\n",
    "    \\item Implement a Gradient Descent Function\n",
    "    \\item Extract the Twitter Features\n",
    "    \\item Train the Model\n",
    "    \\item Test the Logistic Regression\n",
    "    \\item Error Analysis\n",
    "    \\item Predict out of Sample using New Tweets\n",
    "\\end{enumerate}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-procurement",
   "metadata": {},
   "source": [
    "## 1. Import functions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eastern-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import nltk and to call directory location\n",
    "import nltk\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-festival",
   "metadata": {},
   "source": [
    "### Imported functions\n",
    "\n",
    "Download the data needed for this Lab. Check out the [documentation for the twitter_samples dataset](http://www.nltk.org/howto/twitter.html).\n",
    "\n",
    "* twitter_samples: if you're running this notebook on your local computer, you will need to download it using:\n",
    "```Python\n",
    "nltk.download('twitter_samples')\n",
    "```\n",
    "\n",
    "* stopwords: if you're running this notebook on your local computer, you will need to download it using:\n",
    "```python\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "standard-nurse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/JosephNavelski/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/JosephNavelski/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to import nltk dictionaries (only need to run once)\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-devil",
   "metadata": {},
   "source": [
    "#### Import some helper functions that we provided in the utils.py file:\n",
    "* `process_tweet()`: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
    "* `build_freqs()`: this counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0', then builds the `freqs` dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "clean-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/JosephNavelski/../tmp2/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "# this enables importing of these files without downloading it again when we refresh our workspace\n",
    "\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)\n",
    "\n",
    "filePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "surprised-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import twitter_samples \n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-party",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  \n",
    "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
    "    * You will select just the five thousand positive tweets and five thousand negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "funky-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-conflict",
   "metadata": {},
   "source": [
    "* Train test split: 20% will be in the test set, and 80% in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "loaded-supplement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training data = 8000 observations\n",
      "length of test data = 2000 observations\n",
      "['@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!', '@97sides CONGRATS :)', 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days', '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM', \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\", '@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.', 'Jgh , but we have to go to Bayan :D bye', 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']\n",
      "['@GODDAMMlT SRSLY FUCK U UNFOLLOWER HOPE UR FUTURE CHILD UNPARENTS U &gt;:-(', \"Ah i honestly love my dad so much I'm slightly upset that I'm not seeing him on my birthday :(\", '@soarcasm Bianca \\n\\nUr one and only bun : (', \"you already knew i couldn't stand to let people dislike me :(( it's burdensome and i want totally clear it as soon as possible\", '@aquazzSky same here omg I miss them so badly :(((', 'Amelia didnt stalk my twitter :(', 'oh, i missed the broadcast. : (', \"i really can't stream on melon i feel useless :-(\", 'I need to stop looking at old soccer pictures :(', 'Got an interview for the job that I want but they rang me Tuesday for the interview on Thursday but in on holiday :(']\n",
      "------------------------------------------\n",
      "[\"@heyclaireee is back! thnx God!!! i'm so happy :)\", '@BBCRadio3 thought it was my ears which were malfunctioning, thank goodness you cleared that one up with an apology :-)', \"@HumayAG 'Stuck in the centre right with you. Clowns to the right, jokers to the left...' :) @orgasticpotency @ahmedshaheed @AhmedSaeedGahaa\", 'Happy Friday :-) http://t.co/iymPIlWXFY', '@Sazzi91 we are following you now :) x', 'My #TeenChoice For #ChoiceinternationalArtist is #SuperJunior Fighting Oppa :D', \"@FindBenNeedham it's my birthday today so for my birthday wish I hope there's good news about Ben soon :-)\", \"Good morning all :-)\\n\\nIt's Friday!!!!!! \\U000fec00\\n\\nWhat are your plans for the day? I am currently playing shops with my... http://t.co/qoKquDWcb5\", '@LouiseR97054900 Happy Friday for you too :) @toonstra65 @_emeraldeye_  @lisamarti76 @Dahlizma  @miss_steele89 @LouMWrites @ASeguda']\n",
      "['I want it to be my birthday already :(', \"@louanndavies Completely agree. The press won't :(\", 'Im super duper tired :(', \"Having boring time :( don't know what to do.....\", 'ill be on soon, I PROMISE :(\\nwaaah', 'I wanna change my avi but uSanele :(', 'MY PUPPY BROKE HER FOOT :(', \"where's all the jaebum baby pictures :((\", 'But but Mr Ahmad Maslan cooks too :( https://t.co/ArCiD31Zv6', '@eawoman As a Hull supporter I am expecting a misserable few weeks :-(']\n"
     ]
    }
   ],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "print(\"length of training data =\", len(train_x), \"observations\")\n",
    "print(\"length of test data =\", len(test_x), \"observations\")\n",
    "\n",
    "type(train_x)\n",
    "type(test_x)\n",
    "\n",
    "# Look at the data!\n",
    "print(train_x[1:10])\n",
    "print(train_x[(len(train_x)-10):len(train_x)])\n",
    "print(\"------------------------------------------\")\n",
    "print(test_x[1:10])\n",
    "print(test_x[(len(test_x)-10):len(test_x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-cattle",
   "metadata": {},
   "source": [
    "* Create the numpy array of positive labels and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ideal-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extensive-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets (Note: This is just a vector of 1's and 0's)\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-flexibility",
   "metadata": {},
   "source": [
    "* Create the frequency dictionary using the imported `build_freqs()` function.  \n",
    "    * It is recomended to open `utils.py` and read the `build_freqs()` function to understand what it is doing.\n",
    "\n",
    "```Python\n",
    "    for y,tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "```\n",
    "* Notice how the outer for loop goes through each tweet, and the inner for loop steps through each word in a tweet.\n",
    "* The `freqs` dictionary is the frequency dictionary that's being built. \n",
    "* The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0).  The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suspected-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11340\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))\n",
    "\n",
    "# freqs\n",
    "# type(freqs)\n",
    "# freqsdf = pd.DataFrame.from_dict(freqs, orient = 'index',columns=freqs.keys())\n",
    "# str(freqsdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-nickel",
   "metadata": {},
   "source": [
    "### Process tweet using `process_tweet()` function\n",
    "The given function `process_tweet()` tokenizes the tweet into individual words, removes stop words and applies stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entitled-stanley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-johnson",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "This is an example of a positive tweet: \n",
    " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
    " \n",
    "This is an example of the processes version: \n",
    " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-genesis",
   "metadata": {},
   "source": [
    "# 2. Define a Sigmoid for Logistic Regression\n",
    "\n",
    "\n",
    "### Part 1.1: Sigmoid\n",
    "You will learn to use logistic regression for text classification. \n",
    "* The sigmoid function is defined as: \n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability. \n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='../tmp2/sigmoid_plot.jpg.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figure 1 </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
